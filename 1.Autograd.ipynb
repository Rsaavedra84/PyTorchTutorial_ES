{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUTORIAL PYTORCH\n",
    "## Autograd\n",
    "###### Fuente: [Documentación Oficial de Pytorch](https://pytorch.org/tutorials/)\n",
    "###### Edición y traducción por Cristobal Donoso O. \n",
    "###### Agosto 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "La clase principal para todas las redes neuronales es **autograd**. Este paquete nos permite diferenciar automaticamente todas las operaciones en los Tensores. El backprop estará definido según la estructura de nuestro codigo; para cada iteración puede ser diferente.\n",
    "\n",
    "#### Tensor\n",
    "La clase principal de autograd es ```torch.Tensor```. Si inicializas el atributo ```.requires_grad``` como ```True``` entonces comenzará a hacer seguimiento de todas las operaciones en el. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando finalices el *foward* puedes llamar a ```.backwar()``` y obtendras todos los gradientes automaticamente. El gradiente para un tensor será guardado en un atributo ```.grad```<br><br>\n",
    "Para sacar un tensor del historial de seguimiento puedes llamar al metodo ```.detach()```<br>\n",
    "En ocaciones, no es necesario ajustar pesos del modelo (por ejemplo en el testing). Para detener todo el historial de seguimiento basta con colocar ```torch.no_grad()```.\n",
    "\n",
    "#### Función\n",
    "Tensores y Funciones están interconectados. Juntos, construyen el grafo aciclico de operaciones y su historial de seguimiento. Cada Tensor tiene asociado un atributo ```.grad_fn``` que hace referencia a la clase **Function**  que ha creado al tensor. Si el tensor fue creado por el usuario ```.grad_fn``` será ```none```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward>) \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Puntero referencia a Function Class de y: <AddBackward object at 0x7fd048eb02e8>\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y,'\\n','-'*100)\n",
    "print('Puntero referencia a Function Class de y:',y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso la variable ```y``` fué creada como resultado de una operación, por lo tanto tiene un ```.grad_fn```. Podemos seguir realizando operaciones y generaremos más tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward>)\n",
      "tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si existe algun Tensor que **no fue inicializado** con ```requires_grad=True``` simplemente cambiamos el valor por defecto en la clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7fd048eb78d0>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)   #creamos el tensor\n",
    "a = ((a * 3) / (a - 1)) # aplicamos operaciones sobre el\n",
    "print(a.requires_grad)  # vemos si se le está haciendo seguimiento a los gradientes\n",
    "a.requires_grad_(True)  # Cambiamos el valor del atributo\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradientes\n",
    "Como la variable ```out``` contiene un *escalar*, ```out.backward()``` es equivalente con ```out.backward(torch.tensor(1))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matemáticamente,<br>\n",
    "<center>$\n",
    "\\begin{equation}\n",
    "out = \\frac{1}{4}\\sum_i z_i\\\\\n",
    "z_i = 3(y_i)^2\\\\\n",
    "y_i = x_i + 2\\\\\n",
    "\\end{equation}\n",
    "$</center>\n",
    "En este caso\n",
    "<center>$\n",
    "x = \\begin{bmatrix}\n",
    "    1 & 1  \\\\\n",
    "    1 & 1\n",
    "\\end{bmatrix}\n",
    "$</center>\n",
    "<br>entonces la evaluación de cada elemento en x sobre el grafo de operaciones arrojará como resultado:\n",
    "<center>$\n",
    "z_i\\Big|_{x_i=1} = 27\n",
    "$</center>\n",
    "Finalmente hacemos el calculo de los gradientes asociados,\n",
    "<center>$\n",
    "\\frac{\\partial out}{\\partial x_i} = \\frac{3}{2}(x_i+2) \n",
    "$</center>\n",
    "donde \n",
    "<center>\n",
    "    $\\frac{\\partial out}{\\partial x_i} \\Big|_{x_i=1}=\\frac{9}{2} = 4.5$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de la variable\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward>)\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "Valor de los gradientes\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print('Valor de la variable')\n",
    "print(z)\n",
    "print('-'*100,'\\nValor de los gradientes')\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otro ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1405.2299,  -287.5595,  -637.3141])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que acá estamos inicializando los gradientes, puesto que ```y``` no es escalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 102.4000, 1024.0000,    0.1024])\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos detener todo el seguimiento de gradientes usando ```torch.no_grad()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
